{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3453128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9434e803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7141 entries, 0 to 7140\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   post_id      7141 non-null   object\n",
      " 1   comment_id   7141 non-null   object\n",
      " 2   author       7043 non-null   object\n",
      " 3   score        7141 non-null   int64 \n",
      " 4   created_utc  7141 non-null   object\n",
      " 5   body         7140 non-null   object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 334.9+ KB\n",
      "None\n",
      "\n",
      "Missing Values:\n",
      "post_id         0\n",
      "comment_id      0\n",
      "author         98\n",
      "score           0\n",
      "created_utc     0\n",
      "body            1\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Comments (by comment_id):\n",
      "0\n",
      "\n",
      "Unique Posts: 500\n",
      "Unique Authors: 2268\n",
      "\n",
      "Score Statistics:\n",
      "count    7141.000000\n",
      "mean        1.845820\n",
      "std         4.664144\n",
      "min       -25.000000\n",
      "25%         1.000000\n",
      "50%         1.000000\n",
      "75%         2.000000\n",
      "max       132.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "AutoModerator Comments: 499\n",
      "\n",
      "Cleaned data saved to 'reddit_comments_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the data (assuming the CSV content is provided as a string or file)\n",
    "# For demonstration, I'll simulate loading the data you provided\n",
    "data = pd.read_csv('reddit_comments.csv')\n",
    "\n",
    "# Step 2.1: Inspect the data\n",
    "print(\"Dataset Info:\")\n",
    "print(data.info())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(data.isnull().sum())\n",
    "print(\"\\nDuplicate Comments (by comment_id):\")\n",
    "print(data['comment_id'].duplicated().sum())\n",
    "\n",
    "# Step 2.2: Parse created_utc to datetime\n",
    "data['created_utc'] = pd.to_datetime(data['created_utc'])\n",
    "\n",
    "# Step 2.3: Basic EDA\n",
    "# Number of unique posts and authors\n",
    "unique_posts = data['post_id'].nunique()\n",
    "unique_authors = data['author'].nunique()\n",
    "print(f\"\\nUnique Posts: {unique_posts}\")\n",
    "print(f\"Unique Authors: {unique_authors}\")\n",
    "\n",
    "# Score distribution\n",
    "print(\"\\nScore Statistics:\")\n",
    "print(data['score'].describe())\n",
    "\n",
    "# Check for AutoModerator comments (common in Reddit data)\n",
    "automoderator_comments = data[data['author'] == 'AutoModerator'].shape[0]\n",
    "print(f\"\\nAutoModerator Comments: {automoderator_comments}\")\n",
    "\n",
    "# Step 2.4: Save cleaned data for further analysis\n",
    "data.to_csv('reddit_comments_cleaned.csv', index=False)\n",
    "print(\"\\nCleaned data saved to 'reddit_comments_cleaned.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0df21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate comment_ids: 0\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7141 entries, 0 to 7140\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   post_id      7141 non-null   object        \n",
      " 1   comment_id   7141 non-null   object        \n",
      " 2   author       7043 non-null   object        \n",
      " 3   score        7141 non-null   int64         \n",
      " 4   created_utc  7141 non-null   datetime64[ns]\n",
      " 5   body         7141 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(4)\n",
      "memory usage: 334.9+ KB\n",
      "None\n",
      "\n",
      "Missing Values:\n",
      "post_id         0\n",
      "comment_id      0\n",
      "author         98\n",
      "score           0\n",
      "created_utc     0\n",
      "body            0\n",
      "dtype: int64\n",
      "\n",
      "Row Count: 7141\n",
      "\n",
      "Fixed CSV saved to 'reddit_comments_fixed.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "# Step 1: Read the CSV with robust parsing\n",
    "try:\n",
    "    data = pd.read_csv('reddit_comments.csv', quoting=csv.QUOTE_ALL, escapechar='\\\\', encoding='utf-8', on_bad_lines='warn')\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV: {e}\")\n",
    "    # Fallback: Manual parsing\n",
    "    with open('reddit_comments.csv', 'r', encoding='utf-8') as file:\n",
    "        raw_data = file.read()\n",
    "    csv_buffer = StringIO(raw_data)\n",
    "    data = pd.read_csv(csv_buffer, quoting=csv.QUOTE_ALL, escapechar='\\\\', encoding='utf-8', on_bad_lines='warn')\n",
    "\n",
    "# Step 2: Text Preprocessing for body column (NLP-inspired, no NLTK)\n",
    "def clean_body_text(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Encode to handle emojis and non-ASCII characters\n",
    "    text = text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "    # Replace newlines and carriage returns with spaces\n",
    "    text = re.sub(r'[\\n\\r]+', ' ', text)\n",
    "    # Escape double quotes for CSV compatibility\n",
    "    text = text.replace('\"', '\"\"')\n",
    "    # Remove or normalize problematic characters (e.g., multiple spaces, tabs)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Handle common Reddit formatting (e.g., URLs, mentions)\n",
    "    text = re.sub(r'https?://\\S+', '[URL]', text)  # Simplify URLs\n",
    "    text = re.sub(r'u/\\S+', '[USER]', text)  # Simplify user mentions\n",
    "    # Trim leading/trailing spaces\n",
    "    cleaned_text = text.strip()\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply cleaning to body column\n",
    "data['body'] = data['body'].apply(clean_body_text)\n",
    "\n",
    "# Step 3: Validate and clean other columns\n",
    "# Convert created_utc to datetime\n",
    "data['created_utc'] = pd.to_datetime(data['created_utc'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing critical fields\n",
    "data = data.dropna(subset=['comment_id', 'post_id'])\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"Duplicate comment_ids:\", data['comment_id'].duplicated().sum())\n",
    "\n",
    "# Step 4: Basic validation\n",
    "print(\"\\nDataset Info:\")\n",
    "print(data.info())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(data.isnull().sum())\n",
    "print(\"\\nRow Count:\", len(data))\n",
    "\n",
    "# Step 5: Save the corrected CSV\n",
    "data.to_csv('reddit_comments_fixed.csv', index=False, quoting=csv.QUOTE_ALL, escapechar='\\\\', encoding='utf-8')\n",
    "print(\"\\nFixed CSV saved to 'reddit_comments_fixed.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
